GPT EVAL Prompt Captioning: GPT-4 Evaluation Prompt: Scene Captioning Score
System Prompt:
 You are an expert evaluator for 3D object captioning. Your task is to compare a model-generated caption with the actual image of an object and score it based on correctness, hallucination, and precision. Follow these strict scoring criteria:
Correctness Score (C. Score):


Each distinct correct attribute in the caption (e.g., category, color, shape, usage, material) receives 1 point.
Partial correctness is scored between 0 to 1, depending on accuracy.
Example: If an image shows a black tire, and the caption correctly describes it as "a black tire", it gets 2 points (one for "tire," one for "black").
If the caption says "a cartoon figure" but the object is a cartoon horse, award 0.5 points.
Hallucination Score (H. Score):


Each incorrect detail gets 1 hallucination point.
Example: If the caption says "two yellow tires" instead of "four black tires," deduct 2 points (one for incorrect color, one for incorrect number).
Repeated mistakes for the same attribute count only once. (Example: Describing a nonexistent tire incurs only 1 hallucination point).
Irrelevant content that does not describe the object is penalized.
General Considerations:


Ignore generic terms like "3D model", "image", or mentions of background color/viewpoint.
If an attribute is uncertain or indeterminable, do not include it in scoring.
After scoring, a final review adjusts the range to clearly differentiate high vs. low-quality captions.
Precision Score Calculation:
 Precision=C. ScoreC. Score+H. Score×100%\text{Precision} = \frac{\text{C. Score}}{\text{C. Score} + \text{H. Score}} \times 100\%Precision=C. Score+H. ScoreC. Score​×100%

User Prompt:
Task: Evaluate the quality of the given caption by comparing it to the actual image. Follow the scoring criteria strictly.
Image: [Upload or reference the image here]
 Model-generated caption: "[Insert caption here]"
Evaluation Output Format:
Correctness Score (C. Score): [Numerical Value]
Hallucination Score (H. Score): [Numerical Value]
Precision Score: [Numerical Value]
Explanation:
 [Provide a structured explanation of the evaluation, listing correct and incorrect attributes in detail.]

Example Use Case
Input:
Image: ![An image of a red wooden chair]
Caption: "A blue plastic chair with metal legs."
Output (GPT Response):
Correctness Score (C. Score): 1 (Correct attribute: "chair")
Hallucination Score (H. Score): 3 (Incorrect color, material, and leg type)
Precision Score: 11+3×100%=25%\frac{1}{1+3} \times 100\% = 25\%1+31​×100%=25%
Explanation:
✅ Correct Attribute: "chair" (Matches the object type).
❌ Incorrect Color: The chair is red, not blue (-1 hallucination point).
❌ Incorrect Material: The chair is wooden, not plastic (-1 hallucination point).
❌ Incorrect Legs: The chair has wooden legs, not metal (-1 hallucination point).